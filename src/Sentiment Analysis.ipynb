{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import random\n",
    "from glob import glob\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "def read_folder(path_to_folder):\n",
    "    \"\"\" RUN ONCE to read the dataset into numpy arrays\"\"\"\n",
    "    for split in ['train', 'test']:\n",
    "        samples = []\n",
    "        for class_label in ['pos', 'neg']:\n",
    "            fnames = glob(os.path.join(path_to_folder, split, class_label) + '/*.txt')\n",
    "            for fname in fnames:\n",
    "                with open(fname) as fin:\n",
    "                    line = fin.readline()\n",
    "                    samples.append((line, 1 if class_label == 'pos' else 0))\n",
    "        random.shuffle(samples)\n",
    "        samples = np.array(samples)\n",
    "        print(samples.shape)\n",
    "        out_fname = 'train' if split == 'train' else 'test'\n",
    "        np.save(os.path.join(path_to_folder, out_fname + '.npy'), samples)\n",
    "\n",
    "read_folder('aclImdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.load('aclImdb/train.npy')\n",
    "train_corpus = [s[0] for s in train]\n",
    "y = [s[1] for s in train]\n",
    "\n",
    "test = np.load('aclImdb/test.npy')\n",
    "test_corpus = [s[0] for s in test]\n",
    "y_test = [s[1] for s in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "reviews_train_clean = preprocess_reviews(train_corpus)\n",
    "reviews_test_clean = preprocess_reviews(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "english_stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stop_word(corpus):\n",
    "    cleaned = []\n",
    "    for review in corpus:\n",
    "        cleaned.append(\n",
    "            ' '.join([word for word in review.split() \n",
    "                                 if word not in english_stop_words]))\n",
    "    return cleaned\n",
    "\n",
    "removed_train = remove_stop_word(reviews_train_clean)\n",
    "removed_test = remove_stop_word(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stemmed_text(corpus):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "stemmed_reviews_train = get_stemmed_text(removed_train)\n",
    "stemmed_reviews_test = get_stemmed_text(removed_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "def get_lemmatized_text(corpus):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "train = get_lemmatized_text(stemmed_reviews_train)\n",
    "test = get_lemmatized_text(stemmed_reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "english_stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def sentiment(review, vectorizer, model):\n",
    "    # Preprocessing\n",
    "    cleaned = REPLACE_NO_SPACE.sub(NO_SPACE, review.lower())\n",
    "    cleaned = REPLACE_WITH_SPACE.sub(SPACE, cleaned)\n",
    "    removed = ' '.join([word for word in review.split() if word not in english_stop_words])\n",
    "    stemmed = ' '.join([stemmer.stem(word) for word in removed.split()])\n",
    "    lemmatized = ' '.join([lemmatizer.lemmatize(word) for word in stemmed.split()])\n",
    "    \n",
    "    # Predicting\n",
    "    X = vectorizer.transform([lemmatized])\n",
    "    y = model.predict(X)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(train, test):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectorizer.fit(train)\n",
    "    X = tfidf_vectorizer.transform(train)\n",
    "    X_test = tfidf_vectorizer.transform(test)\n",
    "    return X, X_test, tfidf_vectorizer\n",
    "\n",
    "X, X_test, vectorizer = tfidf(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(X, y):\n",
    "    model = LinearSVC(C=0.1)\n",
    "#     model = LogisticRegression(C=1)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "model = training(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.87932\n"
     ]
    }
   ],
   "source": [
    "print (\"Final Accuracy: %s\" % accuracy_score(y_test, model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(model, vectorizer):\n",
    "    pickle.dump(model, open(\"sentiment_analyzer_svm.p\", \"wb\"))\n",
    "    pickle.dump(vectorizer, open(\"vectorizer.p\", \"wb\"))\n",
    "def load_models():\n",
    "    model = pickle.load(open(\"sentiment_analyzer_svm.p\", \"rb\"))\n",
    "    vectorizer = pickle.load(open(\"vectorizer.p\", \"rb\"))\n",
    "    return model, vectorizer\n",
    "\n",
    "save_models(model, vectorizer)\n",
    "model, vectorizer = load_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting sentiment of News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa18eeaf96949bdbdce5ac6312811ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=595028), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5a08184e6e459c85f8de2494808e3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=635671), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2949188276114ffb9999747591fef52e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=547129), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c7a2a9073f4539941e5214d7c54a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=564579), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17bbb28765764bd1bcbfea2b8069fe73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=189978), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_articles():\n",
    "    for i in range(2016, 2021):\n",
    "        df = pd.read_csv('news-{}.csv'.format(i))\n",
    "        # pipeline\n",
    "        df['sentiment-svm'] = df.progress_apply(lambda x: sentiment(x['article'], vectorizer, model), axis=1)\n",
    "        df.to_csv('news-'+str(i)+'.csv', index=False)\n",
    "    \n",
    "predict_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
